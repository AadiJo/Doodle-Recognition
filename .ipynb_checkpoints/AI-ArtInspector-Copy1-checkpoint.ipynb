{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "a5c603f4-cba9-49a8-9398-4aa860d923b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input, decode_predictions\n",
    "import os\n",
    "\n",
    "# Directory paths\n",
    "data_dir = \"data/\"\n",
    "models_dir = \"models/\"\n",
    "\n",
    "# Create the models directory if it doesn't exist\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "# Load categories you're interested in\n",
    "categories = ['alarm-clock', 'airplane', 'apple', 'banana', 'beach', 'bicycle', 'bridge', 'EiffelTower']\n",
    "#categories = ['alarm-clock', 'airplane', 'apple']\n",
    "\n",
    "# Function to load data for a given category\n",
    "def load_data(category):\n",
    "    file_path = f\"{data_dir}{category}.npy\"\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    return data\n",
    "\n",
    "# Resize image to match MobileNet input size\n",
    "def preprocess_image(img):\n",
    "    #img = cv2.resize(img, (224, 224))\n",
    "    #img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    #if len(img.shape) == 2:  # Grayscale image (single channel)\n",
    "    #    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    #change array to two dimenstions\n",
    "    img = np.reshape(img, (28, 28)) \n",
    "    # Resize the image to the target size\n",
    "    #img = cv2.resize(img, (224, 224))\n",
    "    img = cv2.resize(img, (32, 32))\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "    img = np.repeat(img, 3, axis=2)\n",
    "\n",
    "\n",
    "    # Handle batch dimension (depending on your data structure)\n",
    "    #if len(img.shape) == 3:  # Single image, add batch dimension\n",
    "    #    img = np.expand_dims(img, axis=0)  # Add batch dimension for a single image\n",
    "\n",
    "    \n",
    "    img = preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "# Load MobileNet model without top layer\n",
    "# Load weights from the local file\n",
    "#base_model = MobileNet(weights='models/mobilenet_1_0_224_tf_no_top.h5', include_top=False, input_shape=(224, 224,3))\n",
    "base_model = MobileNet(weights='models/mobilenet_1_0_224_tf_no_top.h5', include_top=False, input_shape=(32, 32,3)) # minimum size required\n",
    "\n",
    "# Add classification head\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "predictions = tf.keras.layers.Dense(len(categories), activation='softmax')(x)\n",
    "\n",
    "# Combine base model with classification head\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "# Define the learning rate\n",
    "learning_rate = 0.000001  # Example learning rate value, you can adjust this value as needed\n",
    "\n",
    "# Create the Adam optimizer with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6893e5f-ec28-45d0-b2ba-894f3bc50bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123399, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'alarm-clock'  # Modify this line to specify the category\n",
    "data0 = load_data(category)\n",
    "data0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09bb6eac-d62d-40ae-8e6d-66640b814aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151623, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'airplane'  # Modify this line to specify the category\n",
    "data1 = load_data(category)\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "498d8a34-cd41-45a7-9bb5-370490ffb812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144722, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'apple'  # Modify this line to specify the category\n",
    "data2 = load_data(category)\n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff5bda2b-747f-45ca-a778-c135a42e9a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307936, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'banana'  # Modify this line to specify the category\n",
    "data3 = load_data(category)\n",
    "data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20645111-03f9-4f9d-8631-e793a8ac8cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124938, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'beach'  # Modify this line to specify the category\n",
    "data4 = load_data(category)\n",
    "data4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a12b8565-6340-4392-9e2e-197666242aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126527, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'bicycle'  # Modify this line to specify the category\n",
    "data5 = load_data(category)\n",
    "data5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1caa745-679d-45e4-bf9f-a5a73842078e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133010, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'bridge'  # Modify this line to specify the category\n",
    "data6 = load_data(category)\n",
    "data6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50a47146-32af-44d6-84e1-d9b0d32bf04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134801, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'EiffelTower'  # Modify this line to specify the category\n",
    "data7 = load_data(category)\n",
    "data7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "038c1b9e-24dd-41b4-ace7-a7cc9162a4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960000, 8)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming arr1, arr2, ..., arrN are your NumPy arrays\n",
    "# Each array has the same number of columns but potentially different numbers of rows\n",
    "\n",
    "# Determine the number of rows in each array\n",
    "num_rows0 = data0.shape[0]\n",
    "num_rows1 = data1.shape[0]\n",
    "num_rows2 = data2.shape[0]\n",
    "num_rows3 = data3.shape[0]\n",
    "num_rows4 = data4.shape[0]\n",
    "num_rows5 = data5.shape[0]\n",
    "num_rows6 = data6.shape[0]\n",
    "num_rows7 = data7.shape[0]\n",
    "# Repeat for arr3, arr4, ..., arrN\n",
    "\n",
    "# Choose the number of rows to select from each array (can be the same or different)\n",
    "num_rows_to_select = 120000\n",
    "\n",
    "# Randomly select indices from each array\n",
    "selected_indices_0 = np.random.choice(num_rows0, size=num_rows_to_select, replace=False)\n",
    "selected_indices_1 = np.random.choice(num_rows1, size=num_rows_to_select, replace=False)\n",
    "selected_indices_2 = np.random.choice(num_rows2, size=num_rows_to_select, replace=False)\n",
    "selected_indices_3 = np.random.choice(num_rows3, size=num_rows_to_select, replace=False)\n",
    "selected_indices_4 = np.random.choice(num_rows4, size=num_rows_to_select, replace=False)\n",
    "selected_indices_5 = np.random.choice(num_rows5, size=num_rows_to_select, replace=False)\n",
    "selected_indices_6 = np.random.choice(num_rows6, size=num_rows_to_select, replace=False)\n",
    "selected_indices_7 = np.random.choice(num_rows7, size=num_rows_to_select, replace=False)\n",
    "\n",
    "\n",
    "# Select rows from each array based on the randomly chosen indices\n",
    "selected_rows_0 = data0[selected_indices_0]\n",
    "y_train0 = np.zeros((num_rows_to_select, len(categories)))\n",
    "\n",
    "selected_rows_1 = data1[selected_indices_1]\n",
    "y_train1 = np.zeros((num_rows_to_select, len(categories)))\n",
    "\n",
    "selected_rows_2 = data2[selected_indices_2]\n",
    "y_train2 = np.zeros((num_rows_to_select, len(categories)))\n",
    "\n",
    "selected_rows_3 = data3[selected_indices_3]\n",
    "y_train3 = np.zeros((num_rows_to_select, len(categories)))\n",
    "\n",
    "selected_rows_4 = data4[selected_indices_4]\n",
    "y_train4 = np.zeros((num_rows_to_select, len(categories)))\n",
    "\n",
    "selected_rows_5 = data5[selected_indices_5]\n",
    "y_train5 = np.zeros((num_rows_to_select, len(categories)))\n",
    "\n",
    "selected_rows_6 = data6[selected_indices_6]\n",
    "y_train6 = np.zeros((num_rows_to_select, len(categories)))\n",
    "\n",
    "selected_rows_7 = data7[selected_indices_7]\n",
    "y_train7 = np.zeros((num_rows_to_select, len(categories)))\n",
    "\n",
    "# Concatenate the selected rows from all arrays\n",
    "data = np.concatenate((selected_rows_0, selected_rows_1,selected_rows_2,selected_rows_3,selected_rows_4,selected_rows_5,selected_rows_6, selected_rows_7), axis=0)\n",
    "y_train = np.concatenate((y_train0, y_train1, y_train2, y_train3, y_train4, y_train5, y_train6, y_train7), axis=0)\n",
    "\n",
    "u=0\n",
    "v=0\n",
    "row=0\n",
    "while u< len(categories) :\n",
    "    while v<num_rows_to_select:\n",
    "        row = num_rows_to_select * u\n",
    "        y_train[v+row][u] = 1\n",
    "        v+=1\n",
    "        \n",
    "    u+=1\n",
    "    v=0\n",
    "\n",
    "# Optionally, shuffle the merged array\n",
    "#np.random.shuffle(data)\n",
    "\n",
    "data.shape \n",
    "y_train.shape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "b444f852-9780-48a5-9308-1b84e4b61a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[125099]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a596ba4-3091-4b88-99c3-d7a293ebd0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_small = np.concatenate((selected_rows_0[0:1000], selected_rows_1[0:1000],selected_rows_2[0:1000],selected_rows_3[0:1000],selected_rows_4[0:1000],selected_rows_5[0:1000],selected_rows_6[0:1000], selected_rows_7[0:1000]), axis=0)\n",
    "y_train_small = np.concatenate((y_train0[0:1000], y_train1[0:1000], y_train2[0:1000], y_train3[0:1000], y_train4[0:1000], y_train5[0:1000], y_train6[0:1000], y_train7[0:1000]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df47bd18-964a-412e-8ecd-c3bb2fca40d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_selected = 1000\n",
    "u=0\n",
    "v=0\n",
    "row=0\n",
    "while u< len(categories) :\n",
    "    while v<n_selected:\n",
    "        row =  n_selected * u\n",
    "        y_train_small[v+row][u] = 1\n",
    "        v+=1\n",
    "        \n",
    "    u+=1\n",
    "    v=0\n",
    "\n",
    "y_train_small[799]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72c50a5c-d4c9-45e3-87e0-edd570034ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 784)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff1475e8-6f2a-473b-81bc-819219a5339c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9404754-727a-41b9-9c72-97bacd2cc6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,  91,  24,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   1,  87, 116,  30,\n",
       "         0,   0,   0,   0,   0,   0,   0,  72, 255, 105,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  26, 250,\n",
       "       255, 199,   0,   0,   0,   0,   0,   0,   0, 137, 255, 137,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0, 139, 255, 255,  19,   0,   0,   0,   0,   0,   0,  14, 249,\n",
       "       250, 144,  69,   7,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0, 111, 255, 241,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0, 101, 187, 242, 255, 237,  73,   0,   0,   0,   0,   0,   0,\n",
       "        19,  83, 145, 130, 207, 255, 243,  97,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   6,  68, 199, 251,  63,   0,  12,  73,\n",
       "       139, 206, 253, 255, 255, 255, 255, 199,  13,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   8, 213, 230, 193,\n",
       "       250, 255, 244, 204, 237, 255, 213, 147, 156, 255,  79,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  22, 205,\n",
       "       255, 223, 127,  61,   6, 164, 195,  64,   0,   0,   2, 203, 224,\n",
       "        11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  33,\n",
       "       226, 242, 105,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        52, 252, 136,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        10, 208, 233,  41,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0, 148, 250,  42,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0, 147, 250,  63,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  16, 249, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  16, 244, 148,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0, 234, 142,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0, 113, 255,  43,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 222, 153,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 216, 189,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 210,\n",
       "       166,   0,   0,   0,   0,   0,   0,   0,  37, 255,  95,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         3, 231, 156,   0,   0,   0,   0,   0,   0,   0,  95, 255,  35,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  63, 255,  78,   0,   0,   0,   0,   0,   0,   0, 124,\n",
       "       252,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0, 148, 241,   8,   0,   0,   0,   0,   0,   0,\n",
       "         0, 125, 252,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   3, 231, 163,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0, 125, 252,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 129, 255,  73,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0, 125, 252,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  95, 254, 147,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  88, 255,  84,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  75, 249, 178,   3,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   2, 200, 227,  40,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3, 131, 253, 197,\n",
       "        11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  44,\n",
       "       231, 245, 107,   1,   0,   0,   0,   0,   0,   0,  26, 192, 255,\n",
       "       147,   7,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  20, 174, 255, 198, 113,  36,   0,   0,   0,  70, 233,\n",
       "       240,  83,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,  95, 204, 254, 255, 255, 255, 255,\n",
       "       255, 204,  35,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  26,  98, 119,\n",
       "       119, 119, 105,   7,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_small[899]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf5eb230-7a62-41f8-8fbb-7bbe335f7cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_small[899]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "c3378338-355a-468b-b56c-3f1d81c5383c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (8000, 32, 32, 3) y_train_part shape =  (8000, 8)\n",
      "Epoch 1/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 129ms/step - accuracy: 0.5119 - loss: 1.7706\n",
      "Epoch 2/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 150ms/step - accuracy: 0.8101 - loss: 0.6261\n",
      "Epoch 3/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 138ms/step - accuracy: 0.8536 - loss: 0.4568\n",
      "Epoch 4/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - accuracy: 0.8813 - loss: 0.3975\n",
      "Epoch 5/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 130ms/step - accuracy: 0.8884 - loss: 0.3383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([preprocess_image(img) for img in data_small])  \n",
    "y_train_part= y_train_small\n",
    "#y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(categories))\n",
    "print (\"X_train shape = \", X_train.shape, \"y_train_part shape = \", y_train_part.shape)\n",
    "model.fit(X_train, y_train_part, epochs=5, batch_size=32)\n",
    "\n",
    "model.save(f\"{models_dir}mobilenet_doodle_recognition_8_cat_apr4_32x32_small.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "16b7fbbc-0cb2-439c-9aab-43c7d17ad8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_med = np.concatenate((selected_rows_0[0:50000], selected_rows_1[0:50000],selected_rows_2[0:50000],selected_rows_3[0:50000],selected_rows_4[0:50000],selected_rows_5[0:50000],selected_rows_6[0:50000], selected_rows_7[0:50000]), axis=0)\n",
    "y_train_med = np.concatenate((y_train0[0:50000], y_train1[0:50000], y_train2[0:50000], y_train3[0:50000], y_train4[0:50000], y_train5[0:50000], y_train6[0:50000], y_train7[0:50000]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "3854fd57-9de3-4689-bb07-f6a062f708b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_selected = 50000\n",
    "u=0\n",
    "v=0\n",
    "row=0\n",
    "while u< len(categories) :\n",
    "    while v<n_selected:\n",
    "        row =  n_selected * u\n",
    "        y_train_med[v+row][u] = 1\n",
    "        v+=1\n",
    "        \n",
    "    u+=1\n",
    "    v=0\n",
    "\n",
    "y_train_med[5199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "2814c910-3df2-4f88-938c-2427691e83e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 784)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_med.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "eb1d8fc2-0e09-4d45-b504-c361ce99abc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 8)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_med.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "aa9433ff-29c3-4340-91ce-e6653027731a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_med[11099]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "2524e514-a68f-471e-923b-413fc378e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_XL = np.concatenate((selected_rows_0[0:120000], selected_rows_1[0:120000],selected_rows_2[0:120000],selected_rows_3[0:120000],selected_rows_4[0:120000],selected_rows_5[0:120000],selected_rows_6[0:120000], selected_rows_7[0:120000]), axis=0)\n",
    "y_train_XL = np.concatenate((y_train0[0:120000], y_train1[0:120000], y_train2[0:120000], y_train3[0:120000], y_train4[0:120000], y_train5[0:120000], y_train6[0:120000], y_train7[0:120000]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "ef47a792-1f2b-4f17-bc46-fdf70740517c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_selected = 120000\n",
    "u=0\n",
    "v=0\n",
    "row=0\n",
    "while u< len(categories) :\n",
    "    while v<n_selected:\n",
    "        row =  n_selected * u\n",
    "        y_train_XL[v+row][u] = 1\n",
    "        v+=1\n",
    "        \n",
    "    u+=1\n",
    "    v=0\n",
    "\n",
    "y_train_XL[510099]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "19be68a7-30a7-412a-9fdd-7bf069b0a620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (960000, 32, 32, 3) y_train_part shape =  (960000, 8)\n",
      "Epoch 1/5\n",
      "\u001b[1m30000/30000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4036s\u001b[0m 134ms/step - accuracy: 0.8727 - loss: 0.4152\n",
      "Epoch 2/5\n",
      "\u001b[1m30000/30000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4009s\u001b[0m 134ms/step - accuracy: 0.8873 - loss: 0.3641\n",
      "Epoch 3/5\n",
      "\u001b[1m30000/30000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3965s\u001b[0m 132ms/step - accuracy: 0.8987 - loss: 0.3261\n",
      "Epoch 4/5\n",
      "\u001b[1m30000/30000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3830s\u001b[0m 128ms/step - accuracy: 0.9072 - loss: 0.2991\n",
      "Epoch 5/5\n",
      "\u001b[1m30000/30000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4155s\u001b[0m 138ms/step - accuracy: 0.9128 - loss: 0.2793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([preprocess_image(img) for img in data_XL])  \n",
    "y_train_part= y_train_XL\n",
    "#y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(categories))\n",
    "print (\"X_train shape = \", X_train.shape, \"y_train_part shape = \", y_train_part.shape)\n",
    "model.fit(X_train, y_train_part, epochs=5, batch_size=32)\n",
    "\n",
    "model.save(f\"{models_dir}mobilenet_doodle_recognition_8_cat_apr4_32x32_XL.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ba1bd-1e91-4c1a-b453-b661887b72e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "n = 0\n",
    "low=0\n",
    "high=0\n",
    "while n < 8:\n",
    "    low= n*1000\n",
    "    high = low+ 999\n",
    "    n+=1\n",
    "    print(\"low = \", low, \"high =\", high, \"n= \", n)\n",
    "    X_train = np.array([preprocess_image(img) for img in data_small[low:high]])  \n",
    "    y_train_part= y_train_small[low:high]\n",
    "    #y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(categories))\n",
    "    print (\"X_train shape = \", X_train.shape, \"y_train_part shape = \", y_train_part.shape)\n",
    "    model.fit(X_train, y_train_part, epochs=5, batch_size=32)\n",
    "    model.save(f\"{models_dir}mobilenet_doodle_recognition_8_cat_new_run_{n}.h5\")\n",
    "    model = tf.keras.models.load_model(f'{models_dir}/mobilenet_doodle_recognition_8_cat_new_run_{n}.h5')\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.save(f\"{models_dir}mobilenet_doodle_recognition_8_cat_new.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ade876-397d-4905-909f-084de3aa125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(f'{models_dir}/mobilenet_doodle_recognition_8_cat_new_run_1.h5')\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ee4f0-67c0-4c21-bf06-a8df566ea7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "while n < 8:\n",
    "    low= n*1000\n",
    "    high = low+ 999\n",
    "    n+=1\n",
    "    print(\"low = \", low, \"high =\", high, \"n= \", n)\n",
    "    X_train = np.array([preprocess_image(img) for img in data_small[low:high]])  \n",
    "    y_train_part= y_train_small[low:high]\n",
    "    #y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(categories))\n",
    "    print (\"X_train shape = \", X_train.shape, \"y_train_part shape = \", y_train_part.shape)\n",
    "    model.fit(X_train, y_train_part, epochs=5, batch_size=32)\n",
    "    model.save(f\"{models_dir}mobilenet_doodle_recognition_8_cat_wt_run_{n}.h5\")\n",
    "    model.load_weights(f'{models_dir}/mobilenet_doodle_recognition_8_cat_wt_run_{n}.h5') \n",
    "  \n",
    "\n",
    "model.save(f\"{models_dir}mobilenet_doodle_recognition_8_cat_wt.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "045b0251-c8d3-4ee7-b19c-8dde903bab8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alarm-clock',\n",
       " 'airplane',\n",
       " 'apple',\n",
       " 'banana',\n",
       " 'beach',\n",
       " 'bicycle',\n",
       " 'bridge',\n",
       " 'EiffelTower']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "81753df5-7eb1-4238-bdb6-67060b656087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "new_8_model = tf.keras.models.load_model('models\\mobilenet_doodle_recognition_8_cat_apr4_32x32_large.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "4dcb0d20-17dd-487c-aabf-90803e1b66a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "(32, 32, 3)\n",
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAjSklEQVR4nO3de3BU9f3/8dcmJAuYZDGE3EqCASpUIVgjxlRLqaRA2nFQaQernULr6CDBqdBrOq14ab+xOm21HUpn2g60o4ilLTA6LV6iCVUDLdGIeImQxgLNBaXNbgi5mXx+fzjdX1cDnE+yyyebPB8zZ4bseee97+ORfXGyJ5/1GWOMAAA4xxJcDwAAGJsIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOjHM9wAcNDAyoublZqamp8vl8rscBAFgyxqijo0O5ublKSDj9dc6IC6Dm5mbl5eW5HgMAMExHjx7V1KlTT7s/Zj+C27hxoy644AKNHz9excXF+tvf/ubp+1JTU2M1EgDgHDrb63lMAuixxx7T+vXrtWHDBr300kuaN2+elixZouPHj5/1e/mxGwCMDmd9PTcxcPnll5vy8vLw1/39/SY3N9dUVlae9XuDwaCRxMbGxsYW51swGDzj633Ur4B6e3tVV1en0tLS8GMJCQkqLS1VbW3th+p7enoUCoUiNgDA6Bf1AHr33XfV39+vrKysiMezsrLU2tr6ofrKykoFAoHwxg0IADA2OP89oIqKCgWDwfB29OhR1yMBAM6BqN+GnZGRocTERLW1tUU83tbWpuzs7A/V+/1++f3+aI8BABjhon4FlJycrKKiIlVVVYUfGxgYUFVVlUpKSqL9dACAOBWTX0Rdv369Vq5cqcsuu0yXX365HnzwQXV2duorX/lKLJ4OABCHYhJAK1as0DvvvKM777xTra2tuuSSS7R79+4P3ZgAABi7fMYY43qI/xUKhRQIBFyPAVhJTEy0qr/pppus6idMmOC59p133rHq/eqrr3quffvtt6169/X1WdVjdAkGg0pLSzvtfud3wQEAxiYCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgREzWggNGA5u1C3/84x9b9f74xz9uVW+z1E9PT49V75aWFs+1O3futOr9yCOPeK7t6Oiw6o34xxUQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwgrXgMGYkJSVZ1efm5nquXbZsmVVvn89nVV9fX++5NjMz06r3/PnzPdc2NjZa9R43jpcYnB5XQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATrJMBnEZfX5/n2jfeeMOq90UXXWQ7Tsx0dXV5rg2FQla9u7u7bcfBGMIVEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIK14DBm2KztJklHjhzxXPuLX/zCqvfq1aut6jMyMjzX+v1+q96vvPKK59q6ujqr3jbrzGHs4QoIAOBE1APorrvuks/ni9hmz54d7acBAMS5mPwI7uKLL9Yzzzzz/59kHD/pAwBEikkyjBs3TtnZ2bFoDQAYJWLyHtChQ4eUm5ur6dOn66abbjrjm7k9PT0KhUIRGwBg9It6ABUXF2vLli3avXu3Nm3apKamJn3yk59UR0fHoPWVlZUKBALhLS8vL9ojAQBGoKgHUFlZmb7whS+osLBQS5Ys0Z///Ge1t7fr97///aD1FRUVCgaD4e3o0aPRHgkAMALF/O6ASZMm6cILL9Thw4cH3e/3+61/bwEAEP9i/ntAJ0+eVGNjo3JycmL9VACAOBL1APrGN76hmpoavf3223rxxRd13XXXKTExUV/84hej/VQAgDgW9R/BHTt2TF/84hd14sQJTZkyRVdddZX27t2rKVOmRPupECd8Pp/nWtsfx44fP95zbXJyslVvm1mef/55q962ywJdcsklnmtzc3Oteu/bt89z7T/+8Q+r3vn5+Z5rbf+b2NSf7iao0+nt7bWqN8ZY1eN9UQ+gbdu2RbslAGAUYi04AIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAmfGWGLGIVCIQUCAddjIIps1mubN2+eVe8rrrjCc21hYaFV74svvthz7UUXXWTVOzU11ao+lmxeAoLBoFVvm8/3amxstOp9uo94GYztEmEHDx60qu/p6bGqHyuCwaDS0tJOu58rIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJluKB1VI5knTDDTdY1d9yyy2eay+55BKr3uPGjfNc29bWZtX79ddf91xru3TLK6+8YlUfy6VeJk+e7Lk2Pz/fqveMGTM8186cOdOqt019SkqKVe/q6mqr+kOHDnmuffjhh616//Wvf7WqH0lYigcAMCIRQAAAJwggAIATBBAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIAT3hfSglMTJkywqr/ssss81/7f//2fVe/p06db1R87dsxz7QMPPGDV+7nnnvNce+TIEaveNuuv9fb2xqy3JMVyycbExETPtTZr70lSUlKS59rk5GSr3qmpqZ5rv/zlL1v1vvHGG63qL730Us+1U6dOtepts1bfrl27rHq7XgqUKyAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEa8E5lJKS4rl2/vz5Vr3vvfdez7VpaWlWvX/1q19Z1b/wwguea9966y2r3sePH/dc29XVZdUb51ZCgt2/hydOnOi5NjMz06q37d+JlpYWz7W26+nZrAUXb7gCAgA4YR1Ae/bs0TXXXKPc3Fz5fD7t3LkzYr8xRnfeeadycnI0YcIElZaW6tChQ9GaFwAwSlgHUGdnp+bNm6eNGzcOuv/+++/Xz372M/3yl7/Uvn37dN5552nJkiXq7u4e9rAAgNHD+j2gsrIylZWVDbrPGKMHH3xQ3/ve97Rs2TJJ0u9+9ztlZWVp586duuGGG4Y3LQBg1Ijqe0BNTU1qbW1VaWlp+LFAIKDi4mLV1tYO+j09PT0KhUIRGwBg9ItqALW2tkqSsrKyIh7PysoK7/ugyspKBQKB8JaXlxfNkQAAI5Tzu+AqKioUDAbD29GjR12PBAA4B6IaQNnZ2ZKktra2iMfb2trC+z7I7/crLS0tYgMAjH5RDaCCggJlZ2erqqoq/FgoFNK+fftUUlISzacCAMQ567vgTp48qcOHD4e/bmpqUn19vdLT05Wfn6877rhDP/jBD/TRj35UBQUF+v73v6/c3Fxde+210ZwbABDnrANo//79+vSnPx3+ev369ZKklStXasuWLfrWt76lzs5O3XrrrWpvb9dVV12l3bt3a/z48dGbeoSyWRpEki699FLPtWvWrLHqbXMzx4YNG6x6//Wvf7WqP3bsmOfanp4eq94Y2WyWncnNzbXq/ZWvfMVz7Sc+8Qmr3s8884xV/ZNPPum5trm52ar322+/bVUfT6wDaOHChTLGnHa/z+fTPffco3vuuWdYgwEARjfnd8EBAMYmAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4IT1Ujw4vQsuuMCq/nOf+5znWpt146T3Pwrdq8cee8yqd3d3t1X9mZZuwuj2wQ+nPJPly5db9bZZC+7FF1+06m3z90eS9u7d67m2vb3dqvdoxhUQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4ARL8ZzF+PHjPdcWFRVZ9V6wYIHn2ldffdWq94MPPui5tqury6o3xq6UlBSrepu/EzfddJNV7+bmZs+19957r1XvxsZGq/re3l6reryPKyAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEa8GdRV5enufayy67zKr3xIkTPdc+9thjVr1PnDhhVQ94MWPGDKv6JUuWeK7NyMiw6v21r33Nc+0bb7xh1RvnBldAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMsxXMWn/jEJzzXXnrppVa933rrLc+1u3btsuoNxMKsWbOs6m2Wp6qvr7fqzd+J+McVEADACQIIAOCEdQDt2bNH11xzjXJzc+Xz+bRz586I/atWrZLP54vYli5dGq15AQCjhHUAdXZ2at68edq4ceNpa5YuXaqWlpbw9uijjw5rSADA6GN9E0JZWZnKysrOWOP3+5WdnT3koQAAo19M3gOqrq5WZmamZs2apdtuu+2MH47W09OjUCgUsQEARr+oB9DSpUv1u9/9TlVVVfrRj36kmpoalZWVqb+/f9D6yspKBQKB8GbzCaQAgPgV9d8DuuGGG8J/njt3rgoLCzVjxgxVV1dr0aJFH6qvqKjQ+vXrw1+HQiFCCADGgJjfhj19+nRlZGTo8OHDg+73+/1KS0uL2AAAo1/MA+jYsWM6ceKEcnJyYv1UAIA4Yv0juJMnT0ZczTQ1Nam+vl7p6elKT0/X3XffreXLlys7O1uNjY361re+pZkzZ2rJkiVRHRwAEN+sA2j//v369Kc/Hf76v+/frFy5Ups2bdKBAwf029/+Vu3t7crNzdXixYt17733yu/3R2/qYRg3zu6Q586d67k2OTnZqvezzz7rufbUqVNWvQGvAoGA51rbteBSUlI81/7hD3+w6o34Zx1ACxculDHmtPuffPLJYQ0EABgbWAsOAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcCLqnwc00tl+1pDNR4u/++67Vr1fe+01q3ogFiZOnOi5dsKECVa9u7u7Pde2trZa9Ub84woIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcGLMLcVju5SI3+/3XGuz7IgkBYNBq3ogFhITE2NSK0kDAwOea3t7e616I/5xBQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwYc2vB+Xy+mNUbY6x6v/fee1b1QCx0dXV5rrVd7zApKclzbSAQsOqN+McVEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAODEmFuKx3a5HJv6cePs/nMmJydb1QOxcPLkSc+1//nPf6x62/w/Pnv2bKvejz/+uFU9Rh6ugAAATlgFUGVlpebPn6/U1FRlZmbq2muvVUNDQ0RNd3e3ysvLNXnyZKWkpGj58uVqa2uL6tAAgPhnFUA1NTUqLy/X3r179fTTT6uvr0+LFy9WZ2dnuGbdunV6/PHHtX37dtXU1Ki5uVnXX3991AcHAMQ3qzctdu/eHfH1li1blJmZqbq6Oi1YsEDBYFC/+c1vtHXrVl199dWSpM2bN+tjH/uY9u7dqyuuuCJ6kwMA4tqw3gMKBoOSpPT0dElSXV2d+vr6VFpaGq6ZPXu28vPzVVtbO2iPnp4ehUKhiA0AMPoNOYAGBgZ0xx136Morr9ScOXMkSa2trUpOTtakSZMiarOystTa2jpon8rKSgUCgfCWl5c31JEAAHFkyAFUXl6ugwcPatu2bcMaoKKiQsFgMLwdPXp0WP0AAPFhSL8HtHbtWj3xxBPas2ePpk6dGn48Oztbvb29am9vj7gKamtrU3Z29qC9/H6//H7/UMYAAMQxqysgY4zWrl2rHTt26Nlnn1VBQUHE/qKiIiUlJamqqir8WENDg44cOaKSkpLoTAwAGBWsroDKy8u1detW7dq1S6mpqeH3dQKBgCZMmKBAIKCbb75Z69evV3p6utLS0nT77berpKSEO+AAABGsAmjTpk2SpIULF0Y8vnnzZq1atUqS9NOf/lQJCQlavny5enp6tGTJEv3iF7+IyrAAgNHDKoC8rIs2fvx4bdy4URs3bhzyULFks+6VJHV0dHiuzc/Pt+o9bdo0z7Uvv/yyVW/Aq56eHs+1x44ds+pts3bcVVddZdV7165dnmvfeustq944N1gLDgDgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHBiSB/HEM+am5ut6hsaGjzXFhYWWvX+zGc+47n2ySeftOrd1dVlVQ94cfDgQav6mpoaz7Vf+tKXrHqvW7fOc+2dd95p1fvf//63VX1/f79VPd7HFRAAwAkCCADgBAEEAHCCAAIAOEEAAQCcIIAAAE4QQAAAJwggAIATBBAAwAkCCADgBAEEAHBizK0F995771nVV1dXe66dO3euVe/i4mLPtVdccYVV7+eee86qHvDizTfftKp/+OGHPdfm5ORY9V69erXn2p6eHqveP/zhD63q33nnHat6vI8rIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJnzHGuB7if4VCIQUCAddjhCUkeM/oG2+80ar3XXfd5bk2KSnJqve9997rufbXv/61VW8gFiZPnmxVv379es+13/3ud616P/XUU1b199xzj+fav//971a9e3t7repHkmAwqLS0tNPu5woIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4wVpwUZSRkWFVv2jRIs+15eXlVr1nzZrlufbhhx+26v3QQw9Z1f/rX//yXNvf32/VG6NHYmKiVf2kSZM8165Zs8aq96pVq6zq3333Xc+1v/rVr6x679y5MyZznAusBQcAGJGsAqiyslLz589XamqqMjMzde2116qhoSGiZuHChfL5fBHb6tWrozo0ACD+WQVQTU2NysvLtXfvXj399NPq6+vT4sWL1dnZGVF3yy23qKWlJbzdf//9UR0aABD/xtkU7969O+LrLVu2KDMzU3V1dVqwYEH48YkTJyo7Ozs6EwIARqVhvQcUDAYlSenp6RGPP/LII8rIyNCcOXNUUVGhU6dOnbZHT0+PQqFQxAYAGP2sroD+18DAgO644w5deeWVmjNnTvjxG2+8UdOmTVNubq4OHDigb3/722poaNCf/vSnQftUVlbq7rvvHuoYAIA4NeQAKi8v18GDB/X8889HPH7rrbeG/zx37lzl5ORo0aJFamxs1IwZMz7Up6KiIuKjdUOhkPLy8oY6FgAgTgwpgNauXasnnnhCe/bs0dSpU89YW1xcLEk6fPjwoAHk9/vl9/uHMgYAII5ZBZAxRrfffrt27Nih6upqFRQUnPV76uvrJUk5OTlDGhAAMDpZBVB5ebm2bt2qXbt2KTU1Va2trZKkQCCgCRMmqLGxUVu3btVnP/tZTZ48WQcOHNC6deu0YMECFRYWxuQAAADxySqANm3aJOn9Xzb9X5s3b9aqVauUnJysZ555Rg8++KA6OzuVl5en5cuX63vf+17UBgYAjA6sBRdFCQl2d7XbrGV1ySWXWPVesWKF59qlS5da9a6trbWq3759u+faF154war38ePHPdcODAxY9cboYXtj0+LFi63qv/SlL3munThxolXvP/7xj55rN2/ebNX7nXfesaq3xVpwAIARiQACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADgx5M8DwofZLvXy73//23Ot7RI1Nr3b29utetsuU7JmzZqY9X7zzTc91x4+fNiq94kTJzzX9vb2WvXu6emJWX289n7vvfeseiclJXmuPdNyMIOxXaFs/PjxnmsvvPBCq96DfYzN6SQmJlr1do0rIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4ARrwcUJ2zW46uvrPdf+5z//serd1tZmVf/xj3/cc63tOlkXX3yx59qOjg6r3sFg0HNtLNdIs623XZeuu7s7JnPYztLX12fV22YtuIKCAqveeXl5VvU+n89zbXV1tVXvF1980XPtyZMnrXq7xhUQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4ITPGGNcD/G/QqGQAoGA6zFwBgkJdv9umTlzpufaoqIiq96XXXZZTOaQpIkTJ3quHTfOblWrxMTEmNXHcpZY9rb9b2Lz0mWzrJIknThxwqr+pZde8lxbVVVl1fuVV17xXGt7nLEWDAaVlpZ22v1cAQEAnCCAAABOEEAAACcIIACAEwQQAMAJAggA4AQBBABwggACADhBAAEAnCCAAABOEEAAACdYCw5jhu06ZikpKZ5rbdaNk6TzzjvPqn7ChAkjYhbbuW1mGT9+vFXv/v5+z7UHDhyw6n3o0CGr+q6uLs+1I+wlN6ZYCw4AMCJZBdCmTZtUWFiotLQ0paWlqaSkRH/5y1/C+7u7u1VeXq7JkycrJSVFy5cvV1tbW9SHBgDEP6sAmjp1qu677z7V1dVp//79uvrqq7Vs2TK99tprkqR169bp8ccf1/bt21VTU6Pm5mZdf/31MRkcABDfhv0eUHp6uh544AF9/vOf15QpU7R161Z9/vOflyS9+eab+tjHPqba2lpdccUVnvrxHhBihfeAhj8L7wENjveABhez94D6+/u1bds2dXZ2qqSkRHV1derr61NpaWm4Zvbs2crPz1dtbe1p+/T09CgUCkVsAIDRzzqAXn31VaWkpMjv92v16tXasWOHLrroIrW2tio5OVmTJk2KqM/KylJra+tp+1VWVioQCIS3vLw864MAAMQf6wCaNWuW6uvrtW/fPt12221auXKlXn/99SEPUFFRoWAwGN6OHj065F4AgPhh90NxScnJyZo5c6YkqaioSH//+9/10EMPacWKFert7VV7e3vEVVBbW5uys7NP28/v98vv99tPDgCIa8P+PaCBgQH19PSoqKhISUlJqqqqCu9raGjQkSNHVFJSMtynAQCMMlZXQBUVFSorK1N+fr46Ojq0detWVVdX68knn1QgENDNN9+s9evXKz09XWlpabr99ttVUlLi+Q44AMAYYix89atfNdOmTTPJyclmypQpZtGiReapp54K7+/q6jJr1qwx559/vpk4caK57rrrTEtLi81TmGAwaCSxsbGxscX5FgwGz/h6z1pwAICYYC04AMCIRAABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4MeICaIQtzAAAGKKzvZ6PuADq6OhwPQIAIArO9no+4taCGxgYUHNzs1JTU+Xz+cKPh0Ih5eXl6ejRo2dcWyjecZyjx1g4RonjHG2icZzGGHV0dCg3N1cJCae/zrH+QLpYS0hI0NSpU0+7Py0tbVSf/P/iOEePsXCMEsc52gz3OL0sKj3ifgQHABgbCCAAgBNxE0B+v18bNmyQ3+93PUpMcZyjx1g4RonjHG3O5XGOuJsQAABjQ9xcAQEARhcCCADgBAEEAHCCAAIAOBE3AbRx40ZdcMEFGj9+vIqLi/W3v/3N9UhRddddd8nn80Vss2fPdj3WsOzZs0fXXHONcnNz5fP5tHPnzoj9xhjdeeedysnJ0YQJE1RaWqpDhw65GXYYznacq1at+tC5Xbp0qZthh6iyslLz589XamqqMjMzde2116qhoSGipru7W+Xl5Zo8ebJSUlK0fPlytbW1OZp4aLwc58KFCz90PlevXu1o4qHZtGmTCgsLw79sWlJSor/85S/h/efqXMZFAD322GNav369NmzYoJdeeknz5s3TkiVLdPz4cdejRdXFF1+slpaW8Pb888+7HmlYOjs7NW/ePG3cuHHQ/ffff79+9rOf6Ze//KX27dun8847T0uWLFF3d/c5nnR4znackrR06dKIc/voo4+ewwmHr6amRuXl5dq7d6+efvpp9fX1afHixers7AzXrFu3To8//ri2b9+umpoaNTc36/rrr3c4tT0vxylJt9xyS8T5vP/++x1NPDRTp07Vfffdp7q6Ou3fv19XX321li1bptdee03SOTyXJg5cfvnlpry8PPx1f3+/yc3NNZWVlQ6niq4NGzaYefPmuR4jZiSZHTt2hL8eGBgw2dnZ5oEHHgg/1t7ebvx+v3n00UcdTBgdHzxOY4xZuXKlWbZsmZN5YuX48eNGkqmpqTHGvH/ukpKSzPbt28M1b7zxhpFkamtrXY05bB88TmOM+dSnPmW+9rWvuRsqRs4//3zz61//+pyeyxF/BdTb26u6ujqVlpaGH0tISFBpaalqa2sdThZ9hw4dUm5urqZPn66bbrpJR44ccT1SzDQ1Nam1tTXivAYCARUXF4+68ypJ1dXVyszM1KxZs3TbbbfpxIkTrkcalmAwKElKT0+XJNXV1amvry/ifM6ePVv5+flxfT4/eJz/9cgjjygjI0Nz5sxRRUWFTp065WK8qOjv79e2bdvU2dmpkpKSc3ouR9xipB/07rvvqr+/X1lZWRGPZ2Vl6c0333Q0VfQVFxdry5YtmjVrllpaWnT33Xfrk5/8pA4ePKjU1FTX40Vda2urJA16Xv+7b7RYunSprr/+ehUUFKixsVHf/e53VVZWptraWiUmJroez9rAwIDuuOMOXXnllZozZ46k989ncnKyJk2aFFEbz+dzsOOUpBtvvFHTpk1Tbm6uDhw4oG9/+9tqaGjQn/70J4fT2nv11VdVUlKi7u5upaSkaMeOHbroootUX19/zs7liA+gsaKsrCz858LCQhUXF2vatGn6/e9/r5tvvtnhZBiuG264IfznuXPnqrCwUDNmzFB1dbUWLVrkcLKhKS8v18GDB+P+PcqzOd1x3nrrreE/z507Vzk5OVq0aJEaGxs1Y8aMcz3mkM2aNUv19fUKBoP6wx/+oJUrV6qmpuaczjDifwSXkZGhxMTED92B0dbWpuzsbEdTxd6kSZN04YUX6vDhw65HiYn/nruxdl4lafr06crIyIjLc7t27Vo98cQTeu655yI+NiU7O1u9vb1qb2+PqI/X83m64xxMcXGxJMXd+UxOTtbMmTNVVFSkyspKzZs3Tw899NA5PZcjPoCSk5NVVFSkqqqq8GMDAwOqqqpSSUmJw8li6+TJk2psbFROTo7rUWKioKBA2dnZEec1FApp3759o/q8StKxY8d04sSJuDq3xhitXbtWO3bs0LPPPquCgoKI/UVFRUpKSoo4nw0NDTpy5Ehcnc+zHedg6uvrJSmuzudgBgYG1NPTc27PZVRvaYiRbdu2Gb/fb7Zs2WJef/11c+utt5pJkyaZ1tZW16NFzde//nVTXV1tmpqazAsvvGBKS0tNRkaGOX78uOvRhqyjo8O8/PLL5uWXXzaSzE9+8hPz8ssvm3/+85/GGGPuu+8+M2nSJLNr1y5z4MABs2zZMlNQUGC6urocT27nTMfZ0dFhvvGNb5ja2lrT1NRknnnmGXPppZeaj370o6a7u9v16J7ddtttJhAImOrqatPS0hLeTp06Fa5ZvXq1yc/PN88++6zZv3+/KSkpMSUlJQ6ntne24zx8+LC55557zP79+01TU5PZtWuXmT59ulmwYIHjye185zvfMTU1NaapqckcOHDAfOc73zE+n8889dRTxphzdy7jIoCMMebnP/+5yc/PN8nJyebyyy83e/fudT1SVK1YscLk5OSY5ORk85GPfMSsWLHCHD582PVYw/Lcc88ZSR/aVq5caYx5/1bs73//+yYrK8v4/X6zaNEi09DQ4HboITjTcZ46dcosXrzYTJkyxSQlJZlp06aZW265Je7+8TTY8UkymzdvDtd0dXWZNWvWmPPPP99MnDjRXHfddaalpcXd0ENwtuM8cuSIWbBggUlPTzd+v9/MnDnTfPOb3zTBYNDt4Ja++tWvmmnTppnk5GQzZcoUs2jRonD4GHPuziUfxwAAcGLEvwcEABidCCAAgBMEEADACQIIAOAEAQQAcIIAAgA4QQABAJwggAAAThBAAAAnCCAAgBMEEADACQIIAODE/wM1ZHEFw3R57QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Read the image file\n",
    "img_file = 'images/apple4.png'  # Replace 'your_image.jpg' with the path to your image file\n",
    "\n",
    "img = cv2.imread(img_file)\n",
    "img = cv2.resize(img, (32, 32), interpolation=cv2.INTER_AREA)\n",
    "#img = cv2.bitwise_not(img)\n",
    "#img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the image to single channel if necessary\n",
    "#if len(img.shape) == 3:    \n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "print(img.shape)\n",
    "\n",
    "# Step 2: Reverse the preprocessing steps\n",
    "# Remove extra dimensions\n",
    "#img = np.squeeze(img)\n",
    "img_array = np.array(img)\n",
    "#plt.imshow(img_array.squeeze())\n",
    "#img_array = np.reshape(img_array, (784)) \n",
    "print(img_array.shape)\n",
    "\n",
    "# Resize the image to its original size\n",
    "#img1 = cv2.resize(img_array, (32, 32))#, interpolation=cv2.INTER_AREA)\n",
    "#img1 = cv2.bitwise_not(img1)\n",
    "# Step 3: Convert the image to a NumPy array\n",
    "#img_array = np.array(img)\n",
    "#print(img_array)\n",
    "print(img_array.shape)\n",
    "plt.imshow(img_array.squeeze())\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "#img_array = np.array(preprocess_input(img_array))\n",
    "#img_array= np.expand_dims(img_array, axis=3)\n",
    "#img_array = np.repeat(img_array, 3, axis=3)\n",
    "img_array1 = preprocess_input(img_array)\n",
    "#img_array1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "cbeba345-b759-4f6c-9c9b-31e792f83745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 644ms/step\n",
      "[[3.9205505e-03 6.5811923e-07 9.9162042e-01 4.4443836e-03 6.9222056e-06\n",
      "  6.7226210e-07 5.7939224e-06 5.9704371e-07]]\n",
      "apple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 578ms/step\n",
      "[[1.0241381e-02 6.0046671e-07 9.8658669e-01 3.0202179e-03 1.4945402e-05\n",
      "  4.4485871e-05 6.9814698e-05 2.1898455e-05]]\n",
      "apple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 627ms/step\n",
      "[[1.6156817e-02 6.7051197e-04 7.0172721e-01 1.4925468e-01 4.2912867e-03\n",
      "  1.0163841e-02 1.1216675e-01 5.5688713e-03]]\n",
      "apple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 589ms/step\n",
      "[[9.9934321e-03 3.3025248e-05 9.2757213e-01 5.5391319e-02 1.4009593e-03\n",
      "  4.1446026e-05 5.4987841e-03 6.8879119e-05]]\n",
      "apple\n"
     ]
    }
   ],
   "source": [
    "new_8_model = tf.keras.models.load_model('models\\mobilenet_doodle_recognition_8_cat_apr4_32x32_XL.h5')\n",
    "pred = new_8_model.predict(img_array1)\n",
    "print (pred)\n",
    "print(categories[np.argmax(pred)])\n",
    "\n",
    "new_8_model = tf.keras.models.load_model('models\\mobilenet_doodle_recognition_8_cat_apr4_32x32_large.h5')\n",
    "pred = new_8_model.predict(img_array1)\n",
    "print (pred)\n",
    "print(categories[np.argmax(pred)])\n",
    "\n",
    "new_8_model_med = tf.keras.models.load_model('models\\mobilenet_doodle_recognition_8_cat_apr4_32x32_med.h5')\n",
    "pred = new_8_model_med.predict(img_array1)\n",
    "print (pred)\n",
    "print(categories[np.argmax(pred)])\n",
    "\n",
    "new_8_model_small = tf.keras.models.load_model('models\\mobilenet_doodle_recognition_8_cat_apr4_32x32_small.h5')\n",
    "pred = new_8_model_small.predict(img_array1)\n",
    "print (pred)\n",
    "print(categories[np.argmax(pred)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "e45f1c76-8e47-4412-8344-ad6b8b276cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32)\n",
      "(1, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAgACABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+tLSdB1TXBc/2ZZS3Rto/NlES5Kr64rNora0HwnrfiZpBpVjJMkf+sl6In1Y8CvRdC0vU/BXhjUm0m/trzxBPPCEt9OlEzoquGIO3PpWzqvwisdZ1WfVgNSsYro+cbVbJk8pm5IywwOaoa74c0Dwr4F1rTVtZEvZ4knhurt42aQBwNqbfftVDwF4+8OQaVZaP4lgMUNmH8qQRebExbOGaPkFgT1xXoWq63o2s7jpGvtNbMSVtINWS0VR6FQwP51y8ujC1klnv/C2jWVkihhqGqXZuI3JGQFYkhic9q5Txzc6ff6PDG3iDR3NmCLWx0y1KoMnnkDFebUVoXut6jqGn2djdXcktrZrtgiZvlQewrPr/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACD0lEQVR4AWNgGAKAEdmNfILfXyHzQWwmZAGdJF8xZD46mynn54eZGIJIAsJiX++kJfHBRXg90znhHBBDc9qp8MPXNJhhgjGHdmqhuEFE5MOxyYq+EmAFbDLWHob8PAwsMOVAWpTn7ct9t1wePBMUEhIS5hPS5WFiRyjgkhC3Frn3680p17ivPGysLAyfPx9SY+aAKGAXEuWXVVc2ZHjBwHBCW/nfr0/vP7z98Oymuy/YBCY+FUtbdckf73/deyzJdnMb+z92Xj4RZZH/U96/+80ADEmehDT1l6cOnngkpK2jraMGDLp/39++fs5uNGMn2+lPjAzine5X5l5W8xAzF2f6+uDK2RvPXr/7AnSy8ZTbk08zAN1gpbt3j0YnK9fz81fOXv/465+wqKK4hKSkLL/YC16gOhaG+98MPv8T3Biz8+I3WVUhUVFODk7mX58/X3735vRVsIJ7CwLMWZlUGDREmFj+/fn94+X7Dx8+ffn88cvPP8JCD78BHSls42mlzMn47fXzZy+eP3/y5v17JlnNfwzsAsIib1dfBobk283Xr9qLCjO8ffDs4+df7MJvuPS9Lb8wcnD8+XaNF+RNEGCVsTQwkOFiYgaC/u8uqheesnx6+uDBk/cwBSBFjAIiwsJCItkcfA/nTPsNEsEOep4ej0JKiMixCdGw5vPF/f+x64WI8igIIhmAT+XgkQMAIaO4sENUALkAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from PIL import Image as im \n",
    "img_array = data2[100]\n",
    "img_array = np.reshape(img_array, (28, 28))\n",
    "#img_array = np.resize(img_array, (32,32))\n",
    "img_array = cv2.resize(img_array, (32, 32), interpolation=cv2.INTER_AREA)\n",
    "print(img_array.shape)\n",
    "image = im.fromarray(img_array)\n",
    "# Resize the image to its original size\n",
    "#img1 = cv2.resize(img_array, (32, 32))#, interpolation=cv2.INTER_AREA)\n",
    "#img1 = cv2.bitwise_not(img1)\n",
    "# Step 3: Convert the image to a NumPy array\n",
    "#img_array = np.array(img)\n",
    "#print(img_array)\n",
    "#plt.imshow(img_array.squeeze())\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array = np.array(preprocess_input(img_array))\n",
    "img_array= np.expand_dims(img_array, axis=3)\n",
    "img_array = np.repeat(img_array, 3, axis=3)\n",
    "img_array1 = preprocess_input(img_array)\n",
    "print(img_array1.shape)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "b9f9220b-a36d-4c93-b5ca-e3df534aa565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "[[0.02172538 0.00836168 0.00467332 0.09935816 0.7526157  0.01170942\n",
      "  0.09673721 0.00481904]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'beach'"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_8_model = tf.keras.models.load_model('models\\mobilenet_doodle_recognition_8_cat_apr4_32x32_large.h5')\n",
    "pred = new_8_model.predict(img_array1)\n",
    "print (pred)\n",
    "categories[np.argmax(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "60478631-33e8-4296-8a8f-2211949327d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alarm-clock',\n",
       " 'airplane',\n",
       " 'apple',\n",
       " 'banana',\n",
       " 'beach',\n",
       " 'bicycle',\n",
       " 'bridge',\n",
       " 'EiffelTower']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "36410b23-16f9-4113-9305-4baff9fd34d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APBbW0ub6cQWlvNcTEEiOJC7HHXgV3nhr4VXPiPw22s/2tb2aQTyJdx3ELjyI4yvmMW6ZAYHacZweeK0Y7HwTZeEdU1STw9dm0MT22m319dMJ7y4IxujiUBFRRliTuxwMk15ZXcW9/ceFvhpZ3WlzSW1/rl5Mk13ExWRIYAmI1YcgFpMnHXAFbdp8dvEFto01m+l6TPczNulupITmU7QpZ1BAZsADPt0NZ6+Pk8ZXtvp3jWxs5reTEEd/bQiKezycBlK8MozypHT3rita0qfQ9cvtKuSDNZzvC5HQlTjI9j1rp7GE+JfhtJp9sC+o6DcyXiwgcyWsoQSFR3Ksik+ze1cVXUeFvCjam39ratvs/Dlqd91eONocD/lnHn77tjAA6ZyazPEutP4j8TajrEkYjN5O0oQfwgngfgMCqmn6je6TfRX2n3UtrdRHKSxMVZfxFdMfiTrbnzJbTRJbrvdSaRbtKT6k7OTWJrfiTWfEUyS6tqE11sGI0YgJGPRUGFX8AKyq//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABaElEQVR4AWNgIBkIBuPQIpt34Pe/f3zYZLkm/v1/puflfw80SZ+tiy2Z9v3pUWDQ/ruQG02y4//H/7P/RzAwCNx6JIImx+DyP+v/n7kMDExbvpsgy5mtWdXEpvH/1p9ffAxGR//HI8sF/n1y9u8e2f//n92tWPf3CYo/RN8e5mTY/P/Cz//P/4PA+zXaCK1zvyszMIi3vQeKv1kU65fX9/pvK0xW7l8TmMkVNUcLIsY3+X83VDbxvxJMHZzu+O8PYVf8Z4ILwhhMFy8DmZjiYPl/E3WA7sAhyXCEwQK35O1vOmDJ7wz8MKsQNDvnO7DkCQYbhCCMFcAIchEDA8uniTAhOM1+9wQjhDP/qzxcFMqY8RdmmsTHdWiShf/r4CIFqKmCbeb/ZQgvslx65gRXyVv48G8t1EKwoNqF/8fTVcQExSxTJr37t90WrhLMYM88BYpJIPiwSA8uhdCvrcPPwHj32jO4FAEGAKxKiLyEpZDZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from PIL import Image as im \n",
    "\n",
    "array = data2[100000]\n",
    "print(array.shape) \n",
    "  \n",
    "# Reshape the array into a  \n",
    "# familiar resoluition \n",
    "array = np.reshape(array, (28, 28)) \n",
    "\n",
    "# show the shape of the array \n",
    "print(array.shape) \n",
    "\n",
    "# creating image object of \n",
    "# above array \n",
    "data = im.fromarray(array) \n",
    "  \n",
    "#data.convert(\"L\")\n",
    "data.save(\"images/apple5.png\")\n",
    "data\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc84cbd4-abca-43f0-91dd-019f4342f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = im.open(\"images/airplane2.jpg\")\n",
    "size = (28, 28)\n",
    "img = img.thumbnail(size)\n",
    "img_array = np.array(img)\n",
    "\n",
    "img_array.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc402e8-9926-4bc8-bf45-9f94ce99a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = new_8_model.predict(test_img_array)\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0707075e-a1d5-4607-9518-0ed5994c99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories[np.argmax(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b357c1d5-709b-4e44-8dbb-07b787493f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
