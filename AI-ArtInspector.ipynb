{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "a5c603f4-cba9-49a8-9398-4aa860d923b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input, decode_predictions\n",
    "import os\n",
    "\n",
    "# Directory paths\n",
    "data_dir = \"data/\"\n",
    "models_dir = \"models/\"\n",
    "\n",
    "# Create the models directory if it doesn't exist\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "# Load categories you're interested in\n",
    "categories = ['alarm-clock', 'airplane', 'apple', 'banana', 'beach', 'bicycle', 'bridge', 'EiffelTower']\n",
    "#categories = ['alarm-clock', 'airplane', 'apple']\n",
    "\n",
    "# Function to load data for a given category\n",
    "def load_data(category):\n",
    "    file_path = f\"{data_dir}{category}.npy\"\n",
    "    data = np.load(file_path, allow_pickle=True)\n",
    "    return data\n",
    "\n",
    "# Resize image to match MobileNet input size\n",
    "def preprocess_image(img):\n",
    "    #img = cv2.resize(img, (224, 224))\n",
    "    #img = np.expand_dims(img, axis=0)\n",
    "\n",
    "    #if len(img.shape) == 2:  # Grayscale image (single channel)\n",
    "    #    img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    #change array to two dimenstions\n",
    "    img = np.reshape(img, (28, 28)) \n",
    "    # Resize the image to the target size\n",
    "    #img = cv2.resize(img, (224, 224))\n",
    "    img = cv2.resize(img, (32, 32))\n",
    "    img = np.expand_dims(img, axis=2)\n",
    "    img = np.repeat(img, 3, axis=2)\n",
    "\n",
    "\n",
    "    # Handle batch dimension (depending on your data structure)\n",
    "    #if len(img.shape) == 3:  # Single image, add batch dimension\n",
    "    #    img = np.expand_dims(img, axis=0)  # Add batch dimension for a single image\n",
    "\n",
    "    \n",
    "    img = preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "# Load MobileNet model without top layer\n",
    "# Load weights from the local file\n",
    "#base_model = MobileNet(weights='models/mobilenet_1_0_224_tf_no_top.h5', include_top=False, input_shape=(224, 224,3))\n",
    "base_model = MobileNet(weights='models/mobilenet_1_0_224_tf_no_top.h5', include_top=False, input_shape=(32, 32,3)) # minimum size required\n",
    "\n",
    "# Add classification head\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "predictions = tf.keras.layers.Dense(len(categories), activation='softmax')(x)\n",
    "\n",
    "# Combine base model with classification head\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
    "# Define the learning rate\n",
    "learning_rate = 0.000001  # Example learning rate value, you can adjust this value as needed\n",
    "\n",
    "# Create the Adam optimizer with the specified learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6893e5f-ec28-45d0-b2ba-894f3bc50bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123399, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'alarm-clock'  # Modify this line to specify the category\n",
    "data0 = load_data(category)\n",
    "data0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09bb6eac-d62d-40ae-8e6d-66640b814aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151623, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'airplane'  # Modify this line to specify the category\n",
    "data1 = load_data(category)\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "498d8a34-cd41-45a7-9bb5-370490ffb812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144722, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'apple'  # Modify this line to specify the category\n",
    "data2 = load_data(category)\n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff5bda2b-747f-45ca-a778-c135a42e9a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307936, 784)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'banana'  # Modify this line to specify the category\n",
    "data3 = load_data(category)\n",
    "data3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20645111-03f9-4f9d-8631-e793a8ac8cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124938, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'beach'  # Modify this line to specify the category\n",
    "data4 = load_data(category)\n",
    "data4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a12b8565-6340-4392-9e2e-197666242aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126527, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'bicycle'  # Modify this line to specify the category\n",
    "data5 = load_data(category)\n",
    "data5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1caa745-679d-45e4-bf9f-a5a73842078e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133010, 784)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'bridge'  # Modify this line to specify the category\n",
    "data6 = load_data(category)\n",
    "data6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50a47146-32af-44d6-84e1-d9b0d32bf04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(134801, 784)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'EiffelTower'  # Modify this line to specify the category\n",
    "data7 = load_data(category)\n",
    "data7.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "038c1b9e-24dd-41b4-ace7-a7cc9162a4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960000, 8)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming arr1, arr2, ..., arrN are your NumPy arrays\n",
    "# Each array has the same number of columns but potentially different numbers of rows\n",
    "\n",
    "# Determine the number of rows in each array\n",
    "num_rows0 = data0.shape[0]\n",
    "num_rows1 = data1.shape[0]\n",
    "num_rows2 = data2.shape[0]\n",
    "num_rows3 = data3.shape[0]\n",
    "num_rows4 = data4.shape[0]\n",
    "num_rows5 = data5.shape[0]\n",
    "num_rows6 = data6.shape[0]\n",
    "num_rows7 = data7.shape[0]\n",
    "# Repeat for arr3, arr4, ..., arrN\n",
    "\n",
    "# Choose the number of rows to select from each array (can be the same or different)\n",
    "num_rows_to_select = 120000\n",
    "\n",
    "# Randomly select indices from each array\n",
    "selected_indices_0 = np.random.choice(num_rows0, size=num_rows_to_select, replace=False)\n",
    "selected_indices_1 = np.random.choice(num_rows1, size=num_rows_to_select, replace=False)\n",
    "selected_indices_2 = np.random.choice(num_rows2, size=num_rows_to_select, replace=False)\n",
    "selected_indices_3 = np.random.choice(num_rows3, size=num_rows_to_select, replace=False)\n",
    "selected_indices_4 = np.random.choice(num_rows4, size=num_rows_to_select, replace=False)\n",
    "selected_indices_5 = np.random.choice(num_rows5, size=num_rows_to_select, replace=False)\n",
    "selected_indices_6 = np.random.choice(num_rows6, size=num_rows_to_select, replace=False)\n",
    "selected_indices_7 = np.random.choice(num_rows7, size=num_rows_to_select, replace=False)\n",
    "\n",
    "\n",
    "# Select rows from each array based on the randomly chosen indices\n",
    "selected_rows_0 = data0[selected_indices_0]\n",
    "y_train0 = np.zeros((num_rows_to_select, len(categories)))\n",
    "\n",
    "selected_rows_1 = data1[selected_indices_1]\n",
    "y_train1 = np.zeros((num_rows_to_select, len(categories)))\n",
    "\n",
    "selected_rows_2 = data2[selected_indices_2]\n",
    "y_train2 = np.zeros((num_rows_to_select, len(categories)))\n",
    "\n",
    "selected_rows_3 = data3[selected_indices_3]\n",
    "y_train3 = np.zeros((num_rows_to_select, len(categories)))\n",
    "\n",
    "selected_rows_4 = data4[selected_indices_4]\n",
    "y_train4 = np.zeros((num_rows_to_select, len(categories)))\n",
    "\n",
    "selected_rows_5 = data5[selected_indices_5]\n",
    "y_train5 = np.zeros((num_rows_to_select, len(categories)))\n",
    "\n",
    "selected_rows_6 = data6[selected_indices_6]\n",
    "y_train6 = np.zeros((num_rows_to_select, len(categories)))\n",
    "\n",
    "selected_rows_7 = data7[selected_indices_7]\n",
    "y_train7 = np.zeros((num_rows_to_select, len(categories)))\n",
    "\n",
    "# Concatenate the selected rows from all arrays\n",
    "data = np.concatenate((selected_rows_0, selected_rows_1,selected_rows_2,selected_rows_3,selected_rows_4,selected_rows_5,selected_rows_6, selected_rows_7), axis=0)\n",
    "y_train = np.concatenate((y_train0, y_train1, y_train2, y_train3, y_train4, y_train5, y_train6, y_train7), axis=0)\n",
    "\n",
    "u=0\n",
    "v=0\n",
    "row=0\n",
    "while u< len(categories) :\n",
    "    while v<num_rows_to_select:\n",
    "        row = num_rows_to_select * u\n",
    "        y_train[v+row][u] = 1\n",
    "        v+=1\n",
    "        \n",
    "    u+=1\n",
    "    v=0\n",
    "\n",
    "# Optionally, shuffle the merged array\n",
    "#np.random.shuffle(data)\n",
    "\n",
    "data.shape \n",
    "y_train.shape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "b444f852-9780-48a5-9308-1b84e4b61a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[125099]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a596ba4-3091-4b88-99c3-d7a293ebd0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_small = np.concatenate((selected_rows_0[0:1000], selected_rows_1[0:1000],selected_rows_2[0:1000],selected_rows_3[0:1000],selected_rows_4[0:1000],selected_rows_5[0:1000],selected_rows_6[0:1000], selected_rows_7[0:1000]), axis=0)\n",
    "y_train_small = np.concatenate((y_train0[0:1000], y_train1[0:1000], y_train2[0:1000], y_train3[0:1000], y_train4[0:1000], y_train5[0:1000], y_train6[0:1000], y_train7[0:1000]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df47bd18-964a-412e-8ecd-c3bb2fca40d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_selected = 1000\n",
    "u=0\n",
    "v=0\n",
    "row=0\n",
    "while u< len(categories) :\n",
    "    while v<n_selected:\n",
    "        row =  n_selected * u\n",
    "        y_train_small[v+row][u] = 1\n",
    "        v+=1\n",
    "        \n",
    "    u+=1\n",
    "    v=0\n",
    "\n",
    "y_train_small[799]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72c50a5c-d4c9-45e3-87e0-edd570034ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 784)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff1475e8-6f2a-473b-81bc-819219a5339c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 8)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9404754-727a-41b9-9c72-97bacd2cc6c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,  91,  24,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   1,  87, 116,  30,\n",
       "         0,   0,   0,   0,   0,   0,   0,  72, 255, 105,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  26, 250,\n",
       "       255, 199,   0,   0,   0,   0,   0,   0,   0, 137, 255, 137,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0, 139, 255, 255,  19,   0,   0,   0,   0,   0,   0,  14, 249,\n",
       "       250, 144,  69,   7,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0, 111, 255, 241,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0, 101, 187, 242, 255, 237,  73,   0,   0,   0,   0,   0,   0,\n",
       "        19,  83, 145, 130, 207, 255, 243,  97,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   6,  68, 199, 251,  63,   0,  12,  73,\n",
       "       139, 206, 253, 255, 255, 255, 255, 199,  13,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   8, 213, 230, 193,\n",
       "       250, 255, 244, 204, 237, 255, 213, 147, 156, 255,  79,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  22, 205,\n",
       "       255, 223, 127,  61,   6, 164, 195,  64,   0,   0,   2, 203, 224,\n",
       "        11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  33,\n",
       "       226, 242, 105,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        52, 252, 136,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        10, 208, 233,  41,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0, 148, 250,  42,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0, 147, 250,  63,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  16, 249, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  16, 244, 148,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0, 234, 142,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0, 113, 255,  43,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 222, 153,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 216, 189,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 210,\n",
       "       166,   0,   0,   0,   0,   0,   0,   0,  37, 255,  95,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         3, 231, 156,   0,   0,   0,   0,   0,   0,   0,  95, 255,  35,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  63, 255,  78,   0,   0,   0,   0,   0,   0,   0, 124,\n",
       "       252,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0, 148, 241,   8,   0,   0,   0,   0,   0,   0,\n",
       "         0, 125, 252,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   3, 231, 163,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0, 125, 252,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 129, 255,  73,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0, 125, 252,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  95, 254, 147,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  88, 255,  84,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  75, 249, 178,   3,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   2, 200, 227,  40,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3, 131, 253, 197,\n",
       "        11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  44,\n",
       "       231, 245, 107,   1,   0,   0,   0,   0,   0,   0,  26, 192, 255,\n",
       "       147,   7,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  20, 174, 255, 198, 113,  36,   0,   0,   0,  70, 233,\n",
       "       240,  83,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,  95, 204, 254, 255, 255, 255, 255,\n",
       "       255, 204,  35,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  26,  98, 119,\n",
       "       119, 119, 105,   7,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_small[899]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf5eb230-7a62-41f8-8fbb-7bbe335f7cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_small[899]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "c3378338-355a-468b-b56c-3f1d81c5383c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (8000, 32, 32, 3) y_train_part shape =  (8000, 8)\n",
      "Epoch 1/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 129ms/step - accuracy: 0.5119 - loss: 1.7706\n",
      "Epoch 2/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 150ms/step - accuracy: 0.8101 - loss: 0.6261\n",
      "Epoch 3/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 138ms/step - accuracy: 0.8536 - loss: 0.4568\n",
      "Epoch 4/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 132ms/step - accuracy: 0.8813 - loss: 0.3975\n",
      "Epoch 5/5\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 130ms/step - accuracy: 0.8884 - loss: 0.3383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([preprocess_image(img) for img in data_small])  \n",
    "y_train_part= y_train_small\n",
    "#y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(categories))\n",
    "print (\"X_train shape = \", X_train.shape, \"y_train_part shape = \", y_train_part.shape)\n",
    "model.fit(X_train, y_train_part, epochs=5, batch_size=32)\n",
    "\n",
    "model.save(f\"{models_dir}mobilenet_doodle_recognition_8_cat_apr4_32x32_small.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "16b7fbbc-0cb2-439c-9aab-43c7d17ad8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_med = np.concatenate((selected_rows_0[0:50000], selected_rows_1[0:50000],selected_rows_2[0:50000],selected_rows_3[0:50000],selected_rows_4[0:50000],selected_rows_5[0:50000],selected_rows_6[0:50000], selected_rows_7[0:50000]), axis=0)\n",
    "y_train_med = np.concatenate((y_train0[0:50000], y_train1[0:50000], y_train2[0:50000], y_train3[0:50000], y_train4[0:50000], y_train5[0:50000], y_train6[0:50000], y_train7[0:50000]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "3854fd57-9de3-4689-bb07-f6a062f708b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_selected = 50000\n",
    "u=0\n",
    "v=0\n",
    "row=0\n",
    "while u< len(categories) :\n",
    "    while v<n_selected:\n",
    "        row =  n_selected * u\n",
    "        y_train_med[v+row][u] = 1\n",
    "        v+=1\n",
    "        \n",
    "    u+=1\n",
    "    v=0\n",
    "\n",
    "y_train_med[5199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "2814c910-3df2-4f88-938c-2427691e83e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 784)"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_med.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "eb1d8fc2-0e09-4d45-b504-c361ce99abc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 8)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_med.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "aa9433ff-29c3-4340-91ce-e6653027731a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_med[11099]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "2524e514-a68f-471e-923b-413fc378e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_XL = np.concatenate((selected_rows_0[0:120000], selected_rows_1[0:120000],selected_rows_2[0:120000],selected_rows_3[0:120000],selected_rows_4[0:120000],selected_rows_5[0:120000],selected_rows_6[0:120000], selected_rows_7[0:120000]), axis=0)\n",
    "y_train_XL = np.concatenate((y_train0[0:120000], y_train1[0:120000], y_train2[0:120000], y_train3[0:120000], y_train4[0:120000], y_train5[0:120000], y_train6[0:120000], y_train7[0:120000]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "ef47a792-1f2b-4f17-bc46-fdf70740517c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_selected = 120000\n",
    "u=0\n",
    "v=0\n",
    "row=0\n",
    "while u< len(categories) :\n",
    "    while v<n_selected:\n",
    "        row =  n_selected * u\n",
    "        y_train_XL[v+row][u] = 1\n",
    "        v+=1\n",
    "        \n",
    "    u+=1\n",
    "    v=0\n",
    "\n",
    "y_train_XL[510099]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "19be68a7-30a7-412a-9fdd-7bf069b0a620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (960000, 32, 32, 3) y_train_part shape =  (960000, 8)\n",
      "Epoch 1/5\n",
      "\u001b[1m30000/30000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4036s\u001b[0m 134ms/step - accuracy: 0.8727 - loss: 0.4152\n",
      "Epoch 2/5\n",
      "\u001b[1m30000/30000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4009s\u001b[0m 134ms/step - accuracy: 0.8873 - loss: 0.3641\n",
      "Epoch 3/5\n",
      "\u001b[1m30000/30000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3965s\u001b[0m 132ms/step - accuracy: 0.8987 - loss: 0.3261\n",
      "Epoch 4/5\n",
      "\u001b[1m30000/30000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3830s\u001b[0m 128ms/step - accuracy: 0.9072 - loss: 0.2991\n",
      "Epoch 5/5\n",
      "\u001b[1m30000/30000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4155s\u001b[0m 138ms/step - accuracy: 0.9128 - loss: 0.2793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "X_train = np.array([preprocess_image(img) for img in data_XL])  \n",
    "y_train_part= y_train_XL\n",
    "#y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(categories))\n",
    "print (\"X_train shape = \", X_train.shape, \"y_train_part shape = \", y_train_part.shape)\n",
    "model.fit(X_train, y_train_part, epochs=5, batch_size=32)\n",
    "\n",
    "model.save(f\"{models_dir}mobilenet_doodle_recognition_8_cat_apr4_32x32_XL.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ba1bd-1e91-4c1a-b453-b661887b72e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "n = 0\n",
    "low=0\n",
    "high=0\n",
    "while n < 8:\n",
    "    low= n*1000\n",
    "    high = low+ 999\n",
    "    n+=1\n",
    "    print(\"low = \", low, \"high =\", high, \"n= \", n)\n",
    "    X_train = np.array([preprocess_image(img) for img in data_small[low:high]])  \n",
    "    y_train_part= y_train_small[low:high]\n",
    "    #y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(categories))\n",
    "    print (\"X_train shape = \", X_train.shape, \"y_train_part shape = \", y_train_part.shape)\n",
    "    model.fit(X_train, y_train_part, epochs=5, batch_size=32)\n",
    "    model.save(f\"{models_dir}mobilenet_doodle_recognition_8_cat_new_run_{n}.h5\")\n",
    "    model = tf.keras.models.load_model(f'{models_dir}/mobilenet_doodle_recognition_8_cat_new_run_{n}.h5')\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.save(f\"{models_dir}mobilenet_doodle_recognition_8_cat_new.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ade876-397d-4905-909f-084de3aa125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(f'{models_dir}/mobilenet_doodle_recognition_8_cat_new_run_1.h5')\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ee4f0-67c0-4c21-bf06-a8df566ea7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1\n",
    "while n < 8:\n",
    "    low= n*1000\n",
    "    high = low+ 999\n",
    "    n+=1\n",
    "    print(\"low = \", low, \"high =\", high, \"n= \", n)\n",
    "    X_train = np.array([preprocess_image(img) for img in data_small[low:high]])  \n",
    "    y_train_part= y_train_small[low:high]\n",
    "    #y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(categories))\n",
    "    print (\"X_train shape = \", X_train.shape, \"y_train_part shape = \", y_train_part.shape)\n",
    "    model.fit(X_train, y_train_part, epochs=5, batch_size=32)\n",
    "    model.save(f\"{models_dir}mobilenet_doodle_recognition_8_cat_wt_run_{n}.h5\")\n",
    "    model.load_weights(f'{models_dir}/mobilenet_doodle_recognition_8_cat_wt_run_{n}.h5') \n",
    "  \n",
    "\n",
    "model.save(f\"{models_dir}mobilenet_doodle_recognition_8_cat_wt.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "045b0251-c8d3-4ee7-b19c-8dde903bab8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alarm-clock',\n",
       " 'airplane',\n",
       " 'apple',\n",
       " 'banana',\n",
       " 'beach',\n",
       " 'bicycle',\n",
       " 'bridge',\n",
       " 'EiffelTower']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "81753df5-7eb1-4238-bdb6-67060b656087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "new_8_model = tf.keras.models.load_model('models\\mobilenet_doodle_recognition_8_cat_apr4_32x32_large.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "4dcb0d20-17dd-487c-aabf-90803e1b66a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "(32, 32, 3)\n",
      "(32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Read the image file\n",
    "img_file = 'C:\\\\Users\\\\AadiJ\\\\Downloads\\\\IMG_1442.jpeg'  # Replace 'your_image.jpg' with the path to your image file\n",
    "\n",
    "img = cv2.imread(img_file)\n",
    "img = cv2.resize(img, (32, 32), interpolation=cv2.INTER_AREA)\n",
    "#img = cv2.bitwise_not(img)\n",
    "#img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Convert the image to single channel if necessary\n",
    "#if len(img.shape) == 3:    \n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "print(img.shape)\n",
    "\n",
    "# Step 2: Reverse the preprocessing steps\n",
    "# Remove extra dimensions\n",
    "#img = np.squeeze(img)\n",
    "img_array = np.array(img)\n",
    "#plt.imshow(img_array.squeeze())\n",
    "#img_array = np.reshape(img_array, (784)) \n",
    "print(img_array.shape)\n",
    "\n",
    "# Resize the image to its original size\n",
    "#img1 = cv2.resize(img_array, (32, 32))#, interpolation=cv2.INTER_AREA)\n",
    "#img1 = cv2.bitwise_not(img1)\n",
    "# Step 3: Convert the image to a NumPy array\n",
    "#img_array = np.array(img)\n",
    "#print(img_array)\n",
    "print(img_array.shape)\n",
    "plt.imshow(img_array.squeeze())\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "#img_array = np.array(preprocess_input(img_array))\n",
    "#img_array= np.expand_dims(img_array, axis=3)\n",
    "#img_array = np.repeat(img_array, 3, axis=3)\n",
    "img_array1 = preprocess_input(img_array)\n",
    "#img_array1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "cbeba345-b759-4f6c-9c9b-31e792f83745",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 771ms/step\n",
      "[[0.08108829 0.04152515 0.03643334 0.02263318 0.19998693 0.01038269\n",
      "  0.6045723  0.00337806]]\n",
      "bridge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 890ms/step\n",
      "[[2.5643341e-02 3.2255563e-04 7.7962241e-04 2.1195734e-02 3.8756993e-01\n",
      "  5.8475262e-03 5.5839533e-01 2.4599893e-04]]\n",
      "bridge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 830ms/step\n",
      "[[9.8404139e-03 5.6315406e-04 9.4547140e-06 1.1285210e-02 2.9470908e-02\n",
      "  5.4790120e-04 9.4825310e-01 2.9868454e-05]]\n",
      "bridge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 780ms/step\n",
      "[[0.4420932  0.00239798 0.01029405 0.02470589 0.26593143 0.09121618\n",
      "  0.1543373  0.00902397]]\n",
      "alarm-clock\n"
     ]
    }
   ],
   "source": [
    "new_8_model = tf.keras.models.load_model('models\\mobilenet_doodle_recognition_8_cat_apr4_32x32_XL.h5')\n",
    "pred = new_8_model.predict(img_array1)\n",
    "print (pred)\n",
    "print(categories[np.argmax(pred)])\n",
    "\n",
    "new_8_model = tf.keras.models.load_model('models\\mobilenet_doodle_recognition_8_cat_apr4_32x32_large.h5')\n",
    "pred = new_8_model.predict(img_array1)\n",
    "print (pred)\n",
    "print(categories[np.argmax(pred)])\n",
    "\n",
    "new_8_model_med = tf.keras.models.load_model('models\\mobilenet_doodle_recognition_8_cat_apr4_32x32_med.h5')\n",
    "pred = new_8_model_med.predict(img_array1)\n",
    "print (pred)\n",
    "print(categories[np.argmax(pred)])\n",
    "\n",
    "new_8_model_small = tf.keras.models.load_model('models\\mobilenet_doodle_recognition_8_cat_apr4_32x32_small.h5')\n",
    "pred = new_8_model_small.predict(img_array1)\n",
    "print (pred)\n",
    "print(categories[np.argmax(pred)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "f8b1e6b6-246f-4ae2-95c2-10fbfda20b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  4,  0,\n",
       "         0,  0,  0,  0, 20, 15,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  4,  0,\n",
       "         0,  0,  0,  0, 20, 15,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  4,  0,\n",
       "         0,  0,  0,  0, 20, 15,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array[0][0:32][0:32][0:32][5:20][0].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "e45f1c76-8e47-4412-8344-ad6b8b276cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32)\n",
      "(1, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAgACABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+tLSdB1TXBc/2ZZS3Rto/NlES5Kr64rNora0HwnrfiZpBpVjJMkf+sl6In1Y8CvRdC0vU/BXhjUm0m/trzxBPPCEt9OlEzoquGIO3PpWzqvwisdZ1WfVgNSsYro+cbVbJk8pm5IywwOaoa74c0Dwr4F1rTVtZEvZ4knhurt42aQBwNqbfftVDwF4+8OQaVZaP4lgMUNmH8qQRebExbOGaPkFgT1xXoWq63o2s7jpGvtNbMSVtINWS0VR6FQwP51y8ujC1klnv/C2jWVkihhqGqXZuI3JGQFYkhic9q5Txzc6ff6PDG3iDR3NmCLWx0y1KoMnnkDFebUVoXut6jqGn2djdXcktrZrtgiZvlQewrPr/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAACD0lEQVR4AWNgGAKAEdmNfILfXyHzQWwmZAGdJF8xZD46mynn54eZGIJIAsJiX++kJfHBRXg90znhHBBDc9qp8MPXNJhhgjGHdmqhuEFE5MOxyYq+EmAFbDLWHob8PAwsMOVAWpTn7ct9t1wePBMUEhIS5hPS5WFiRyjgkhC3Frn3680p17ivPGysLAyfPx9SY+aAKGAXEuWXVVc2ZHjBwHBCW/nfr0/vP7z98Oymuy/YBCY+FUtbdckf73/deyzJdnMb+z92Xj4RZZH/U96/+80ADEmehDT1l6cOnngkpK2jraMGDLp/39++fs5uNGMn2+lPjAzine5X5l5W8xAzF2f6+uDK2RvPXr/7AnSy8ZTbk08zAN1gpbt3j0YnK9fz81fOXv/465+wqKK4hKSkLL/YC16gOhaG+98MPv8T3Biz8+I3WVUhUVFODk7mX58/X3735vRVsIJ7CwLMWZlUGDREmFj+/fn94+X7Dx8+ffn88cvPP8JCD78BHSls42mlzMn47fXzZy+eP3/y5v17JlnNfwzsAsIib1dfBobk283Xr9qLCjO8ffDs4+df7MJvuPS9Lb8wcnD8+XaNF+RNEGCVsTQwkOFiYgaC/u8uqheesnx6+uDBk/cwBSBFjAIiwsJCItkcfA/nTPsNEsEOep4ej0JKiMixCdGw5vPF/f+x64WI8igIIhmAT+XgkQMAIaO4sENUALkAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from PIL import Image as im \n",
    "img_array = data2[100]\n",
    "img_array = np.reshape(img_array, (28, 28))\n",
    "#img_array = np.resize(img_array, (32,32))\n",
    "img_array = cv2.resize(img_array, (32, 32), interpolation=cv2.INTER_AREA)\n",
    "print(img_array.shape)\n",
    "image = im.fromarray(img_array)\n",
    "# Resize the image to its original size\n",
    "#img1 = cv2.resize(img_array, (32, 32))#, interpolation=cv2.INTER_AREA)\n",
    "#img1 = cv2.bitwise_not(img1)\n",
    "# Step 3: Convert the image to a NumPy array\n",
    "#img_array = np.array(img)\n",
    "#print(img_array)\n",
    "#plt.imshow(img_array.squeeze())\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array = np.array(preprocess_input(img_array))\n",
    "img_array= np.expand_dims(img_array, axis=3)\n",
    "img_array = np.repeat(img_array, 3, axis=3)\n",
    "img_array1 = preprocess_input(img_array)\n",
    "print(img_array1.shape)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "b9f9220b-a36d-4c93-b5ca-e3df534aa565",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 719ms/step\n",
      "[[0.02172538 0.00836168 0.00467332 0.09935816 0.7526157  0.01170942\n",
      "  0.09673721 0.00481904]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'beach'"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_8_model = tf.keras.models.load_model('models\\mobilenet_doodle_recognition_8_cat_apr4_32x32_large.h5')\n",
    "pred = new_8_model.predict(img_array1)\n",
    "print (pred)\n",
    "categories[np.argmax(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "60478631-33e8-4296-8a8f-2211949327d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alarm-clock',\n",
       " 'airplane',\n",
       " 'apple',\n",
       " 'banana',\n",
       " 'beach',\n",
       " 'bicycle',\n",
       " 'bridge',\n",
       " 'EiffelTower']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "36410b23-16f9-4113-9305-4baff9fd34d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APBbW0ub6cQWlvNcTEEiOJC7HHXgV3nhr4VXPiPw22s/2tb2aQTyJdx3ELjyI4yvmMW6ZAYHacZweeK0Y7HwTZeEdU1STw9dm0MT22m319dMJ7y4IxujiUBFRRliTuxwMk15ZXcW9/ceFvhpZ3WlzSW1/rl5Mk13ExWRIYAmI1YcgFpMnHXAFbdp8dvEFto01m+l6TPczNulupITmU7QpZ1BAZsADPt0NZ6+Pk8ZXtvp3jWxs5reTEEd/bQiKezycBlK8MozypHT3rita0qfQ9cvtKuSDNZzvC5HQlTjI9j1rp7GE+JfhtJp9sC+o6DcyXiwgcyWsoQSFR3Ksik+ze1cVXUeFvCjam39ratvs/Dlqd91eONocD/lnHn77tjAA6ZyazPEutP4j8TajrEkYjN5O0oQfwgngfgMCqmn6je6TfRX2n3UtrdRHKSxMVZfxFdMfiTrbnzJbTRJbrvdSaRbtKT6k7OTWJrfiTWfEUyS6tqE11sGI0YgJGPRUGFX8AKyq//Z",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABaElEQVR4AWNgIBkIBuPQIpt34Pe/f3zYZLkm/v1/puflfw80SZ+tiy2Z9v3pUWDQ/ruQG02y4//H/7P/RzAwCNx6JIImx+DyP+v/n7kMDExbvpsgy5mtWdXEpvH/1p9ffAxGR//HI8sF/n1y9u8e2f//n92tWPf3CYo/RN8e5mTY/P/Cz//P/4PA+zXaCK1zvyszMIi3vQeKv1kU65fX9/pvK0xW7l8TmMkVNUcLIsY3+X83VDbxvxJMHZzu+O8PYVf8Z4ILwhhMFy8DmZjiYPl/E3WA7sAhyXCEwQK35O1vOmDJ7wz8MKsQNDvnO7DkCQYbhCCMFcAIchEDA8uniTAhOM1+9wQjhDP/qzxcFMqY8RdmmsTHdWiShf/r4CIFqKmCbeb/ZQgvslx65gRXyVv48G8t1EKwoNqF/8fTVcQExSxTJr37t90WrhLMYM88BYpJIPiwSA8uhdCvrcPPwHj32jO4FAEGAKxKiLyEpZDZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from PIL import Image as im \n",
    "\n",
    "array = data2[100000]\n",
    "print(array.shape) \n",
    "  \n",
    "# Reshape the array into a  \n",
    "# familiar resoluition \n",
    "array = np.reshape(array, (28, 28)) \n",
    "\n",
    "# show the shape of the array \n",
    "print(array.shape) \n",
    "\n",
    "# creating image object of \n",
    "# above array \n",
    "data = im.fromarray(array) \n",
    "  \n",
    "#data.convert(\"L\")\n",
    "data.save(\"images/apple5.png\")\n",
    "data\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc84cbd4-abca-43f0-91dd-019f4342f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = im.open(\"images/airplane2.jpg\")\n",
    "size = (28, 28)\n",
    "img = img.thumbnail(size)\n",
    "img_array = np.array(img)\n",
    "\n",
    "img_array.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc402e8-9926-4bc8-bf45-9f94ce99a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = new_8_model.predict(test_img_array)\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0707075e-a1d5-4607-9518-0ed5994c99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories[np.argmax(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b357c1d5-709b-4e44-8dbb-07b787493f87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
